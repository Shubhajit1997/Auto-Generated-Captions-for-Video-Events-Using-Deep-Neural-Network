{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1oI-iMQtikZFn6zY-kqxBhsztc5fXasPG","authorship_tag":"ABX9TyNBFb/N3wOzjTUfpH63tAes"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rg0xY3P-IZfk","executionInfo":{"status":"ok","timestamp":1624020748044,"user_tz":-330,"elapsed":406,"user":{"displayName":"40_Shubhajit Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6Xr4a6rIopjhhzqy4NepAxac-oxOzmtx4XDnr=s64","userId":"12357897221958594690"}},"outputId":"af4dc5d8-a381-4fe3-b437-cf1df6710a22"},"source":["!pwd"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PUcmN8i_Ia7g","executionInfo":{"status":"ok","timestamp":1624020750329,"user_tz":-330,"elapsed":664,"user":{"displayName":"40_Shubhajit Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6Xr4a6rIopjhhzqy4NepAxac-oxOzmtx4XDnr=s64","userId":"12357897221958594690"}},"outputId":"abb77c08-6c6d-4f96-82cb-2eb0b4c2505c"},"source":["%cd /content/drive/MyDrive/PROJECTS/VIDEO_CAPTIONING/TENSORFLOW_IMPLEMENTATION"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/PROJECTS/VIDEO_CAPTIONING/TENSORFLOW_IMPLEMENTATION\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RDhh72CEJNZ0","executionInfo":{"status":"ok","timestamp":1624020754602,"user_tz":-330,"elapsed":2628,"user":{"displayName":"40_Shubhajit Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6Xr4a6rIopjhhzqy4NepAxac-oxOzmtx4XDnr=s64","userId":"12357897221958594690"}}},"source":["import functools\n","import operator\n","import os\n","import time\n","\n","import joblib\n","import numpy as np\n","from keras.layers import Input, LSTM, Dense\n","from keras.models import Model, load_model\n","import config "],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"fjjSS8M2GX5h","executionInfo":{"status":"ok","timestamp":1624020756931,"user_tz":-330,"elapsed":408,"user":{"displayName":"40_Shubhajit Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6Xr4a6rIopjhhzqy4NepAxac-oxOzmtx4XDnr=s64","userId":"12357897221958594690"}}},"source":["class VideoDescriptionInference(object):\n","    \"\"\"\n","        Initialize the parameters for the model\n","        \"\"\"\n","    def __init__(self, config):\n","        self.latent_dim = config.latent_dim\n","        self.num_encoder_tokens = config.num_encoder_tokens\n","        self.num_decoder_tokens = config.num_decoder_tokens\n","        self.time_steps_encoder = config.time_steps_encoder\n","        self.max_probability = config.max_probability\n","\n","        # models\n","        self.encoder_model = None\n","        self.decoder_model = None\n","        self.inf_encoder_model = None\n","        self.inf_decoder_model = None\n","        self.save_model_path = config.save_model_path\n","        self.test_path = config.test_path\n","        self.search_type = config.search_type\n","        self.tokenizer = None\n","        self.num = 0\n","\n","    def load_inference_models(self):\n","        # load tokenizer\n","\n","        with open(os.path.join(self.save_model_path, 'tokenizer' + str(self.num_decoder_tokens)), 'rb') as file:\n","            self.tokenizer = joblib.load(file)\n","\n","        # inference encoder model\n","        self.inf_encoder_model = load_model(os.path.join(self.save_model_path, 'encoder_model.h5'))\n","\n","        # inference decoder model\n","        decoder_inputs = Input(shape=(None, self.num_decoder_tokens))\n","        decoder_dense = Dense(self.num_decoder_tokens, activation='softmax')\n","        decoder_lstm = LSTM(self.latent_dim, return_sequences=True, return_state=True)\n","\n","        decoder_state_input_h = Input(shape=(self.latent_dim,))\n","        decoder_state_input_c = Input(shape=(self.latent_dim,))\n","        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","\n","        decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n","        decoder_states = [state_h, state_c]\n","        decoder_outputs = decoder_dense(decoder_outputs)\n","\n","        self.inf_decoder_model = Model(\n","            [decoder_inputs] + decoder_states_inputs,\n","            [decoder_outputs] + decoder_states)\n","        self.inf_decoder_model.load_weights(os.path.join(self.save_model_path, 'decoder_model_weights.h5'))\n","\n","    def greedy_search(self, f):\n","        \"\"\"\n","                :param f: the loaded numpy array after creating videos to frames and extracting features\n","                :return: the final sentence which has been predicted greedily\n","                \"\"\"\n","        inv_map = self.index_to_word()\n","        states_value = self.inf_encoder_model.predict(f.reshape(-1, 80, 4096))\n","        target_seq = np.zeros((1, 1, 1500))\n","        sentence = ''\n","        target_seq[0, 0, self.tokenizer.word_index['bos']] = 1\n","        for i in range(15):\n","            output_tokens, h, c = self.inf_decoder_model.predict([target_seq] + states_value)\n","            states_value = [h, c]\n","            output_tokens = output_tokens.reshape(self.num_decoder_tokens)\n","            y_hat = np.argmax(output_tokens)\n","            if y_hat == 0:\n","                continue\n","            if inv_map[y_hat] is None:\n","                break\n","            else:\n","                sentence = sentence + inv_map[y_hat] + ' '\n","                target_seq = np.zeros((1, 1, 1500))\n","                target_seq[0, 0, y_hat] = 1\n","        return ' '.join(sentence.split()[:-1])\n","\n","    def index_to_word(self):\n","        # inverts word tokenizer\n","        index_to_word = {value: key for key, value in self.tokenizer.word_index.items()}\n","        return index_to_word\n","\n","    def get_test_data(self):\n","        \"\"\"\n","        loads all the numpy files\n","        :return: two lists containing all the video arrays and the video Id\n","        \"\"\"\n","        X_test = []\n","        X_test_filename = []\n","        with open(os.path.join(self.test_path, 'testing_id.txt')) as testing_file:\n","            lines = testing_file.readlines()\n","            for filename in lines:\n","                filename = filename.strip()\n","                f = np.load(os.path.join(self.test_path, 'feat', filename + '.npy'))\n","                X_test.append(f)\n","                X_test_filename.append(filename[:-4])\n","            X_test = np.array(X_test)\n","        return X_test, X_test_filename\n","\n","    def test(self):\n","        \"\"\"\n","            writes the captions of all the testing videos in a text file\n","        \"\"\"\n","        X_test, X_test_filename = self.get_test_data()\n","\n","        # generate inference test outputs\n","        with open(os.path.join(self.test_path, 'test_%s.txt' % self.search_type), 'w') as file:\n","            for idx, x in enumerate(X_test):\n","                file.write(X_test_filename[idx] + ',')\n","                start = time.time()\n","                decoded_sentence = self.greedy_search(x.reshape(-1, 80, 4096))\n","                file.write(decoded_sentence+'\\n')\n","\n","                # re-init max prob\n","                self.max_probability = -1"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VMGY0wguGC2y","executionInfo":{"status":"ok","timestamp":1624020914373,"user_tz":-330,"elapsed":155890,"user":{"displayName":"40_Shubhajit Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6Xr4a6rIopjhhzqy4NepAxac-oxOzmtx4XDnr=s64","userId":"12357897221958594690"}},"outputId":"15b0159a-5b20-48ad-81af-c5e9790efea5"},"source":["if __name__ == \"__main__\":\n","    video_to_text = VideoDescriptionInference(config)\n","    video_to_text.load_inference_models()\n","    video_to_text.test()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"],"name":"stdout"}]}]}